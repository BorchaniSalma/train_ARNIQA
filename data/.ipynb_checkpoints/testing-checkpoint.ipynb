{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775928a9-b089-4d07-a2dc-2efd8cc73fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "class DECAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DECAM dataset class used for pre-training the encoders for IQA.\n",
    "\n",
    "    Args:\n",
    "        root (string): root directory of the dataset\n",
    "        patch_size (int): size of the patches to extract from the images\n",
    "        max_distortions (int): maximum number of distortions to apply to the images\n",
    "        num_levels (int): number of levels of distortion to apply to the images\n",
    "        pristine_prob (float): probability of not distorting the images\n",
    "\n",
    "    Returns:\n",
    "        dictionary with keys:\n",
    "            img_A_orig (Tensor): first view of the image pair\n",
    "            img_A_ds (Tensor): downsampled version of the first view of the image pair (scale factor 2)\n",
    "            img_B_orig (Tensor): second view of the image pair\n",
    "            img_B_ds (Tensor): downsampled version of the second view of the image pair (scale factor 2)\n",
    "            img_A_name (string): name of the image of the first view of the image pair\n",
    "            img_B_name (string): name of the image of the second view of the image pair\n",
    "            distortion_functions (list): list of the names of the distortion functions applied to the images\n",
    "            distortion_values (list): list of the values of the distortion functions applied to the images\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 patch_size: int = 224,\n",
    "                 max_distortions: int = 4,\n",
    "                 num_levels: int = 5,\n",
    "                 pristine_prob: float = 0.05):\n",
    "\n",
    "        root = Path(root)\n",
    "\n",
    "        \n",
    "        filenames_csv_path = \"../../data/decam_dr10_good_exp.csv\"\n",
    "        exp_df = pd.read_csv(filenames_csv_path, header=None, names=[\"expnum\"])\n",
    "\n",
    "        # Path to FITS file containing image data\n",
    "        file_path = \"/global/cfs/cdirs/cosmo/work/legacysurvey/dr10/survey-ccds-decam-dr10.fits.gz\"\n",
    "        image_table = Table.read(file_path)\n",
    "\n",
    "        # List to store paths of selected images\n",
    "        self.ref_images = []\n",
    "\n",
    "        test = image_table[:400]\n",
    "        idx = np.isin(test[\"expnum\"], exp_df[\"expnum\"])\n",
    "        matched_exp = test[idx]\n",
    "        self.ref_images =  matched_exp[\"image_filename\"]\n",
    "        self.hdu_numbers = matched_exp['image_hdu']\n",
    "\n",
    "\n",
    "        # Convert paths to Path objects\n",
    "        self.ref_images = [Path(\"/global/cfs/cdirs/cosmo/work/legacysurvey/dr10/images/\",path) for path in self.ref_images]\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.max_distortions = max_distortions\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.num_levels = num_levels\n",
    "        self.pristine_prob = pristine_prob\n",
    "\n",
    "        assert 0 <= self.max_distortions <= 7, \"The parameter max_distortions must be in the range [0, 7]\"\n",
    "        assert 1 <= self.num_levels <= 5, \"The parameter num_levels must be in the range [1, 5]\"\n",
    "        \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        print(index)\n",
    "        \n",
    "        img_A_path = self.ref_images[index]\n",
    "        print(img_A_path)\n",
    "        hdul_A = fits.open(img_A_path)\n",
    "        img_A = hdul_A[0].data\n",
    "        print(img_A)\n",
    "        # Select another exposure randomly\n",
    "        other_exp_index = np.random.choice(np.setdiff1d(range(len(self.ref_images)), [index]))\n",
    "        img_B_path = self.ref_images[other_exp_index]\n",
    "        hdul_B = fits.open(img_B_path)\n",
    "        img_B = hdul_B[0].data\n",
    "        \n",
    "        # Resize and crop\n",
    "        img_A_orig = resize_crop(img_A, self.patch_size)\n",
    "        img_B_orig = resize_crop(img_B, self.patch_size)\n",
    "\n",
    "        img_A_orig = transforms.ToTensor()(img_A_orig)\n",
    "        img_B_orig = transforms.ToTensor()(img_B_orig)\n",
    "\n",
    "        distort_functions_A = []\n",
    "        distort_values_A = []\n",
    "        distort_functions_B = []\n",
    "        distort_values_B = []\n",
    "\n",
    "        # Distort images with (1 - self.pristine_prob) probability for image A\n",
    "        if random.random() > self.pristine_prob and self.max_distortions > 0:\n",
    "            img_A_orig, distort_functions_A, distort_values_A = distort_images(img_A_orig,\n",
    "                                                                                 max_distortions=self.max_distortions,\n",
    "                                                                                 num_levels=self.num_levels)\n",
    "\n",
    "        # Distort images with (1 - self.pristine_prob) probability for image B\n",
    "        if random.random() > self.pristine_prob and self.max_distortions > 0:\n",
    "            img_B_orig, distort_functions_B, distort_values_B = distort_images(img_B_orig,\n",
    "                                                                                 max_distortions=self.max_distortions,\n",
    "                                                                                 num_levels=self.num_levels)\n",
    "\n",
    "        img_A_orig = self.normalize(img_A_orig)\n",
    "        img_B_orig = self.normalize(img_B_orig)\n",
    "\n",
    "        # Pad to make the length of distort_functions and distort_values equal for all samples\n",
    "        distort_functions_A = [f.__name__ for f in distort_functions_A]\n",
    "        distort_functions_A += [\"\"] * (self.max_distortions - len(distort_functions_A))\n",
    "        distort_values_A += [torch.inf] * (self.max_distortions - len(distort_values_A))\n",
    "\n",
    "        distort_functions_B = [f.__name__ for f in distort_functions_B]\n",
    "        distort_functions_B += [\"\"] * (self.max_distortions - len(distort_functions_B))\n",
    "        distort_values_B += [torch.inf] * (self.max_distortions - len(distort_values_B))\n",
    "\n",
    "        return {\n",
    "            \"img_A_orig\": img_A_orig,\"img_B_orig\": img_B_orig,\n",
    "            \"distortion_functions_A\": distort_functions_A, \"distortion_values_A\": distort_values_A,\n",
    "            \"distortion_functions_B\": distort_functions_B, \"distortion_values_B\": distort_values_B\n",
    "        }\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ref_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d761b8da-e680-4497-990b-773395958749",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "/global/cfs/cdirs/cosmo/work/legacysurvey/dr10/images/decam/CP/V3.1.2/CP20140825/c4d_140825_015328_ooi_i_v1.fits.fz\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resize_crop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Initialize the training dataset and dataloader\u001b[39;00m\n\u001b[1;32m     12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m DECAMDataset(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/global/cfs/cdirs/cosmo/work/legacysurvey/dr10/images\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                     patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                     max_distortions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     15\u001b[0m                                     num_levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     16\u001b[0m                                     pristine_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDECAMDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mDECAMDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m img_B \u001b[38;5;241m=\u001b[39m hdul_B[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Resize and crop\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m img_A_orig \u001b[38;5;241m=\u001b[39m \u001b[43mresize_crop\u001b[49m(img_A, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size)\n\u001b[1;32m     97\u001b[0m img_B_orig \u001b[38;5;241m=\u001b[39m resize_crop(img_B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size)\n\u001b[1;32m     99\u001b[0m img_A_orig \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(img_A_orig)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resize_crop' is not defined"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize the training dataset and dataloader\n",
    "train_dataset = DECAMDataset(root=\"/global/cfs/cdirs/cosmo/work/legacysurvey/dr10/images\",\n",
    "                                    patch_size=224,\n",
    "                                    max_distortions=4,\n",
    "                                    num_levels=5,\n",
    "                                    pristine_prob=0.05)\n",
    "train_dataloader = DECAMDataset.__getitem__(train_dataset, index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fff9b-fbe9-4da8-a7ba-712941fcb044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0352e7-62ca-4803-aa2d-2bd93b805e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bd4ea7-b799-4c57-b5c4-d9a73d5dd646",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DECAMDataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[43mtrain_dataset\u001b[49m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader = DECAMDataset.__getitem__(train_dataset, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74872bf3-ab04-4862-b0c0-e44f905df574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
